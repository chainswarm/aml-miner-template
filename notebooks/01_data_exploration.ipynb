{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "**Purpose**: Explore ingested data from ClickHouse before training\n",
    "\n",
    "This notebook helps you understand:\n",
    "- Data volume and distribution\n",
    "- Time series patterns\n",
    "- Class balance\n",
    "- Missing values\n",
    "- Feature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from packages.training import FeatureExtractor, FeatureBuilder, ModelTrainer\n",
    "from packages.storage import ClientFactory, get_connection_params\n",
    "from notebook_utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK = 'ethereum'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-03-31'\n",
    "WINDOW_DAYS = 7\n",
    "\n",
    "print(f\"Network: {NETWORK}\")\n",
    "print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Window: {WINDOW_DAYS} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to ClickHouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params = get_connection_params(NETWORK)\n",
    "client_factory = ClientFactory(connection_params)\n",
    "\n",
    "print(\"Connected to ClickHouse\")\n",
    "print(f\"Database: {connection_params['database']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Raw Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with client_factory.client_context() as client:\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        alert_id,\n",
    "        created_at,\n",
    "        severity,\n",
    "        source,\n",
    "        alert_hash,\n",
    "        addresses\n",
    "    FROM raw_alerts\n",
    "    WHERE network = '{NETWORK}'\n",
    "        AND created_at BETWEEN '{START_DATE}' AND '{END_DATE}'\n",
    "    ORDER BY created_at\n",
    "    \"\"\"\n",
    "    \n",
    "    alerts_df = client.query_df(query)\n",
    "\n",
    "print(f\"Total alerts: {len(alerts_df):,}\")\n",
    "alerts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alert Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Alert Statistics:\")\n",
    "print(f\"Total Alerts: {len(alerts_df):,}\")\n",
    "print(f\"Date Range: {alerts_df['created_at'].min()} to {alerts_df['created_at'].max()}\")\n",
    "print(f\"Unique Sources: {alerts_df['source'].nunique()}\")\n",
    "print(f\"\\nSeverity Distribution:\")\n",
    "print(alerts_df['severity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts_df['date'] = pd.to_datetime(alerts_df['created_at']).dt.date\n",
    "daily_counts = alerts_df.groupby('date').size()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "daily_counts.plot(kind='line', marker='o')\n",
    "plt.title('Alert Volume Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Alerts')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_counts = alerts_df['severity'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "severity_counts.plot(kind='bar')\n",
    "plt.title('Alert Severity Distribution')\n",
    "plt.xlabel('Severity')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSeverity Percentages:\")\n",
    "print((severity_counts / len(alerts_df) * 100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Raw Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with client_factory.client_context() as client:\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM raw_features\n",
    "    WHERE network = '{NETWORK}'\n",
    "        AND timestamp BETWEEN '{START_DATE}' AND '{END_DATE}'\n",
    "    LIMIT 10000\n",
    "    \"\"\"\n",
    "    \n",
    "    features_df = client.query_df(query)\n",
    "\n",
    "print(f\"Total features sampled: {len(features_df):,}\")\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values:\")\n",
    "missing = features_df.isnull().sum()\n",
    "missing_pct = (missing / len(features_df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(features_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'address' in numeric_cols:\n",
    "    numeric_cols.remove('address')\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_cols)}\")\n",
    "print(numeric_cols[:10])\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    features_df[numeric_cols[:6]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) >= 6:\n",
    "    plot_feature_distributions(features_df, numeric_cols[:6])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Raw Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with client_factory.client_context() as client:\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        cluster_id,\n",
    "        created_at,\n",
    "        alert_count,\n",
    "        address_count,\n",
    "        severity_distribution\n",
    "    FROM raw_clusters\n",
    "    WHERE network = '{NETWORK}'\n",
    "        AND created_at BETWEEN '{START_DATE}' AND '{END_DATE}'\n",
    "    LIMIT 5000\n",
    "    \"\"\"\n",
    "    \n",
    "    clusters_df = client.query_df(query)\n",
    "\n",
    "print(f\"Total clusters sampled: {len(clusters_df):,}\")\n",
    "clusters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster Statistics:\")\n",
    "print(f\"Total Clusters: {len(clusters_df):,}\")\n",
    "print(f\"\\nAlert Count Distribution:\")\n",
    "print(clusters_df['alert_count'].describe())\n",
    "print(f\"\\nAddress Count Distribution:\")\n",
    "print(clusters_df['address_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(clusters_df['alert_count'], bins=50, edgecolor='black')\n",
    "axes[0].set_title('Cluster Alert Count Distribution')\n",
    "axes[0].set_xlabel('Alert Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].hist(clusters_df['address_count'], bins=50, edgecolor='black')\n",
    "axes[1].set_title('Cluster Address Count Distribution')\n",
    "axes[1].set_xlabel('Address Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) >= 5:\n",
    "    sample_features = features_df[numeric_cols[:10]].copy()\n",
    "    sample_features = sample_features.fillna(0)\n",
    "    \n",
    "    plot_correlation_matrix(sample_features, figsize=(10, 8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Key Findings**:\n",
    "\n",
    "1. **Data Volume**: Review total alerts, features, and clusters\n",
    "2. **Time Patterns**: Check for seasonality or trends\n",
    "3. **Severity Distribution**: Understand class balance\n",
    "4. **Data Quality**: Identify missing values and outliers\n",
    "5. **Correlations**: Detect highly correlated features\n",
    "\n",
    "**Next Steps**:\n",
    "- Proceed to Feature Analysis notebook\n",
    "- Address data quality issues if found\n",
    "- Consider class balancing strategies if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}