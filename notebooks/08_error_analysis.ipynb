{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "**Purpose**: Analyze model mistakes to identify improvement opportunities\n",
    "\n",
    "This notebook provides:\n",
    "- False positive analysis\n",
    "- False negative analysis\n",
    "- Error patterns by features\n",
    "- Misclassified example inspection\n",
    "- Threshold optimization\n",
    "- Segment-specific performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from packages.training import FeatureExtractor, FeatureBuilder, ModelTrainer\n",
    "from packages.storage import ClientFactory, get_connection_params\n",
    "from notebook_utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK = 'ethereum'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-02-29'\n",
    "WINDOW_DAYS = 7\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "print(f\"Network: {NETWORK}\")\n",
    "print(f\"Analysis Period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Classification Threshold: {THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params = get_connection_params(NETWORK)\n",
    "client_factory = ClientFactory(connection_params)\n",
    "\n",
    "with client_factory.client_context() as client:\n",
    "    extractor = FeatureExtractor(client)\n",
    "    data = extractor.extract_training_data(\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        window_days=WINDOW_DAYS\n",
    "    )\n",
    "\n",
    "builder = FeatureBuilder()\n",
    "X, y = builder.build_training_features(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Data loaded: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer(model_type='alert_scorer')\n",
    "model, metrics = trainer.train(X_train, y_train, cv_folds=5)\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Model trained with AUC: {metrics['test_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONFUSION MATRIX BREAKDOWN\")\n",
    "print(\"=\"*50)\n",
    "print(f\"True Negatives (TN):  {tn:6d} (Correctly predicted low risk)\")\n",
    "print(f\"False Positives (FP): {fp:6d} (Incorrectly predicted high risk)\")\n",
    "print(f\"False Negatives (FN): {fn:6d} (Incorrectly predicted low risk)\")\n",
    "print(f\"True Positives (TP):  {tp:6d} (Correctly predicted high risk)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Errors: {fp + fn} ({(fp + fn) / len(y_test) * 100:.2f}%)\")\n",
    "print(f\"FP Rate: {fp / (fp + tn) * 100:.2f}%\")\n",
    "print(f\"FN Rate: {fn / (fn + tp) * 100:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, labels=['Low Risk', 'High Risk'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Misclassified Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred,\n",
    "    'probability': y_pred_proba\n",
    "}, index=y_test.index)\n",
    "\n",
    "error_df['error_type'] = 'Correct'\n",
    "error_df.loc[(error_df['actual'] == 0) & (error_df['predicted'] == 1), 'error_type'] = 'False Positive'\n",
    "error_df.loc[(error_df['actual'] == 1) & (error_df['predicted'] == 0), 'error_type'] = 'False Negative'\n",
    "\n",
    "print(\"\\nError Distribution:\")\n",
    "print(error_df['error_type'].value_counts())\n",
    "print(f\"\\nAccuracy: {(error_df['error_type'] == 'Correct').sum() / len(error_df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Positive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_indices = error_df[error_df['error_type'] == 'False Positive'].index\n",
    "tn_indices = error_df[(error_df['actual'] == 0) & (error_df['predicted'] == 0)].index\n",
    "\n",
    "print(f\"\\nAnalyzing {len(fp_indices)} False Positives\")\n",
    "print(f\"Comparing with {len(tn_indices)} True Negatives\")\n",
    "\n",
    "fp_scores = error_df.loc[fp_indices, 'probability']\n",
    "print(f\"\\nFalse Positive Score Statistics:\")\n",
    "print(f\"  Mean: {fp_scores.mean():.4f}\")\n",
    "print(f\"  Median: {fp_scores.median():.4f}\")\n",
    "print(f\"  Min: {fp_scores.min():.4f}\")\n",
    "print(f\"  Max: {fp_scores.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(error_df.loc[tn_indices, 'probability'], bins=30, alpha=0.6, \n",
    "         label='True Negatives', edgecolor='black')\n",
    "plt.hist(fp_scores, bins=30, alpha=0.6, \n",
    "         label='False Positives', edgecolor='black')\n",
    "plt.axvline(x=THRESHOLD, color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('False Positive Score Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([error_df.loc[tn_indices, 'probability'], fp_scores],\n",
    "            labels=['True Negatives', 'False Positives'])\n",
    "plt.axhline(y=THRESHOLD, color='r', linestyle='--', label='Threshold')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.title('Score Comparison Box Plot')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Negative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_indices = error_df[error_df['error_type'] == 'False Negative'].index\n",
    "tp_indices = error_df[(error_df['actual'] == 1) & (error_df['predicted'] == 1)].index\n",
    "\n",
    "print(f\"\\nAnalyzing {len(fn_indices)} False Negatives\")\n",
    "print(f\"Comparing with {len(tp_indices)} True Positives\")\n",
    "\n",
    "fn_scores = error_df.loc[fn_indices, 'probability']\n",
    "print(f\"\\nFalse Negative Score Statistics:\")\n",
    "print(f\"  Mean: {fn_scores.mean():.4f}\")\n",
    "print(f\"  Median: {fn_scores.median():.4f}\")\n",
    "print(f\"  Min: {fn_scores.min():.4f}\")\n",
    "print(f\"  Max: {fn_scores.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(error_df.loc[tp_indices, 'probability'], bins=30, alpha=0.6,\n",
    "         label='True Positives', edgecolor='black')\n",
    "plt.hist(fn_scores, bins=30, alpha=0.6,\n",
    "         label='False Negatives', edgecolor='black')\n",
    "plt.axvline(x=THRESHOLD, color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('False Negative Score Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([error_df.loc[tp_indices, 'probability'], fn_scores],\n",
    "            labels=['True Positives', 'False Negatives'])\n",
    "plt.axhline(y=THRESHOLD, color='r', linestyle='--', label='Threshold')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.title('Score Comparison Box Plot')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Patterns in Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = X_test.columns[:10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    if len(fp_indices) > 0:\n",
    "        axes[idx].hist(X_test.loc[tn_indices, feature], bins=20, alpha=0.5, \n",
    "                      label='TN', edgecolor='black')\n",
    "        axes[idx].hist(X_test.loc[fp_indices, feature], bins=20, alpha=0.5,\n",
    "                      label='FP', edgecolor='black')\n",
    "    axes[idx].set_title(f'{feature}', fontsize=9)\n",
    "    axes[idx].set_xlabel('Value', fontsize=8)\n",
    "    axes[idx].set_ylabel('Count', fontsize=8)\n",
    "    axes[idx].legend(fontsize=7)\n",
    "    axes[idx].tick_params(labelsize=7)\n",
    "\n",
    "plt.suptitle('Feature Distributions: True Negatives vs False Positives')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassification Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(fp_indices) > 0:\n",
    "    print(\"\\n=== HIGH CONFIDENCE FALSE POSITIVES ===\")\n",
    "    print(\"(Low risk alerts incorrectly predicted as high risk)\\n\")\n",
    "    \n",
    "    high_conf_fp = error_df.loc[fp_indices].nlargest(5, 'probability')\n",
    "    for idx, row in high_conf_fp.iterrows():\n",
    "        print(f\"Sample {idx}:\")\n",
    "        print(f\"  Predicted Probability: {row['probability']:.4f}\")\n",
    "        print(f\"  Actual: Low Risk, Predicted: High Risk\")\n",
    "        print(f\"  Top 5 Feature Values:\")\n",
    "        sample_features = X_test.loc[idx].nlargest(5)\n",
    "        for feat, val in sample_features.items():\n",
    "            print(f\"    {feat}: {val:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(fn_indices) > 0:\n",
    "    print(\"\\n=== HIGH CONFIDENCE FALSE NEGATIVES ===\")\n",
    "    print(\"(High risk alerts incorrectly predicted as low risk)\\n\")\n",
    "    \n",
    "    high_conf_fn = error_df.loc[fn_indices].nsmallest(5, 'probability')\n",
    "    for idx, row in high_conf_fn.iterrows():\n",
    "        print(f\"Sample {idx}:\")\n",
    "        print(f\"  Predicted Probability: {row['probability']:.4f}\")\n",
    "        print(f\"  Actual: High Risk, Predicted: Low Risk\")\n",
    "        print(f\"  Top 5 Feature Values:\")\n",
    "        sample_features = X_test.loc[idx].nlargest(5)\n",
    "        for feat, val in sample_features.items():\n",
    "            print(f\"    {feat}: {val:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.1, 0.9, 0.02)\n",
    "fp_rates = []\n",
    "fn_rates = []\n",
    "accuracies = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba > thresh).astype(int)\n",
    "    cm_thresh = confusion_matrix(y_test, y_pred_thresh)\n",
    "    tn_t, fp_t, fn_t, tp_t = cm_thresh.ravel()\n",
    "    \n",
    "    fp_rate = fp_t / (fp_t + tn_t) if (fp_t + tn_t) > 0 else 0\n",
    "    fn_rate = fn_t / (fn_t + tp_t) if (fn_t + tp_t) > 0 else 0\n",
    "    accuracy = (tp_t + tn_t) / len(y_test)\n",
    "    \n",
    "    fp_rates.append(fp_rate)\n",
    "    fn_rates.append(fn_rate)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, fp_rates, label='False Positive Rate', linewidth=2)\n",
    "plt.plot(thresholds, fn_rates, label='False Negative Rate', linewidth=2)\n",
    "plt.plot(thresholds, accuracies, label='Accuracy', linewidth=2)\n",
    "plt.axvline(x=THRESHOLD, color='r', linestyle='--', label=f'Current Threshold ({THRESHOLD})')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Error Rates vs Classification Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_acc_idx = np.argmax(accuracies)\n",
    "print(f\"\\nBest threshold for accuracy: {thresholds[best_acc_idx]:.2f}\")\n",
    "print(f\"  Accuracy: {accuracies[best_acc_idx]:.4f}\")\n",
    "print(f\"  FP Rate: {fp_rates[best_acc_idx]:.4f}\")\n",
    "print(f\"  FN Rate: {fn_rates[best_acc_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error by Score Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df['score_bin'] = pd.cut(error_df['probability'], bins=10, labels=False)\n",
    "\n",
    "bin_analysis = error_df.groupby('score_bin').agg({\n",
    "    'error_type': lambda x: (x != 'Correct').sum(),\n",
    "    'actual': 'count'\n",
    "}).rename(columns={'error_type': 'errors', 'actual': 'total'})\n",
    "\n",
    "bin_analysis['error_rate'] = bin_analysis['errors'] / bin_analysis['total']\n",
    "bin_analysis['bin_center'] = [(i + 0.5) / 10 for i in bin_analysis.index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bin_analysis['bin_center'], bin_analysis['error_rate'], width=0.08, alpha=0.7)\n",
    "plt.xlabel('Predicted Probability Bin')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Error Rate by Score Bin')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nError Analysis by Score Bin:\")\n",
    "print(bin_analysis[['total', 'errors', 'error_rate']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "if len(X_test.columns) > 0:\n",
    "    top_feature = X_test.columns[0]\n",
    "    feature_median = X_test[top_feature].median()\n",
    "    \n",
    "    segments = {\n",
    "        f'{top_feature} < median': X_test[top_feature] < feature_median,\n",
    "        f'{top_feature} >= median': X_test[top_feature] >= feature_median\n",
    "    }\n",
    "    \n",
    "    segment_performance = []\n",
    "    for seg_name, seg_mask in segments.items():\n",
    "        if seg_mask.sum() > 0:\n",
    "            y_seg = y_test[seg_mask]\n",
    "            y_pred_seg = y_pred[seg_mask]\n",
    "            \n",
    "            segment_performance.append({\n",
    "                'Segment': seg_name,\n",
    "                'Size': seg_mask.sum(),\n",
    "                'Accuracy': accuracy_score(y_seg, y_pred_seg),\n",
    "                'Precision': precision_score(y_seg, y_pred_seg, zero_division=0),\n",
    "                'Recall': recall_score(y_seg, y_pred_seg, zero_division=0),\n",
    "                'F1': f1_score(y_seg, y_pred_seg, zero_division=0)\n",
    "            })\n",
    "    \n",
    "    segment_df = pd.DataFrame(segment_performance)\n",
    "    print(\"\\nPerformance by Segment:\")\n",
    "    print(segment_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ERROR ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal Test Samples: {len(y_test)}\")\n",
    "print(f\"Correct Predictions: {(error_df['error_type'] == 'Correct').sum()}\")\n",
    "print(f\"Total Errors: {(error_df['error_type'] != 'Correct').sum()}\")\n",
    "print(f\"\\nError Breakdown:\")\n",
    "print(f\"  False Positives: {len(fp_indices)} ({len(fp_indices)/len(y_test)*100:.2f}%)\")\n",
    "print(f\"  False Negatives: {len(fn_indices)} ({len(fn_indices)/len(y_test)*100:.2f}%)\")\n",
    "print(f\"\\nError Impact:\")\n",
    "print(f\"  FP Rate (of actual negatives): {fp/(fp+tn)*100:.2f}%\")\n",
    "print(f\"  FN Rate (of actual positives): {fn/(fn+tp)*100:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Key Error Patterns**:\n",
    "\n",
    "1. **False Positives**: Review common characteristics\n",
    "2. **False Negatives**: Identify missed high-risk cases\n",
    "3. **Threshold Impact**: Understand FP/FN trade-off\n",
    "4. **Score Confidence**: Analyze errors by prediction confidence\n",
    "\n",
    "**Improvement Opportunities**:\n",
    "- Feature engineering to better distinguish error cases\n",
    "- Threshold adjustment for business requirements\n",
    "- Segment-specific models if performance varies significantly\n",
    "- Additional data collection for challenging cases\n",
    "\n",
    "**Next Steps**:\n",
    "- Implement threshold based on business priorities (FP vs FN cost)\n",
    "- Consider ensemble methods to reduce specific error types\n",
    "- Review misclassified samples for data quality issues\n",
    "- Monitor error patterns in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}