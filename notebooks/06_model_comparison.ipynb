{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "**Purpose**: Compare multiple trained models to select the best one\n",
    "\n",
    "This notebook provides:\n",
    "- Side-by-side metric comparison\n",
    "- Statistical significance tests\n",
    "- Model ensemble experiments\n",
    "- Performance visualization\n",
    "- Best model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from packages.training import FeatureExtractor, FeatureBuilder, ModelTrainer, ModelStorage\n",
    "from packages.storage import ClientFactory, get_connection_params\n",
    "from notebook_utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy import stats\n",
    "\n",
    "setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK = 'ethereum'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-02-29'\n",
    "WINDOW_DAYS = 7\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Network: {NETWORK}\")\n",
    "print(f\"Comparison Period: {START_DATE} to {END_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params = get_connection_params(NETWORK)\n",
    "client_factory = ClientFactory(connection_params)\n",
    "\n",
    "with client_factory.client_context() as client:\n",
    "    extractor = FeatureExtractor(client)\n",
    "    data = extractor.extract_training_data(\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        window_days=WINDOW_DAYS\n",
    "    )\n",
    "\n",
    "builder = FeatureBuilder()\n",
    "X, y = builder.build_training_features(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Data loaded: Train={X_train.shape}, Test={X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multiple Model Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "predictions = {}\n",
    "all_metrics = {}\n",
    "\n",
    "model_configs = {\n",
    "    'Baseline': {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100},\n",
    "    'Deep Trees': {'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 100},\n",
    "    'More Trees': {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300},\n",
    "    'Conservative': {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200},\n",
    "    'Aggressive': {'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 150}\n",
    "}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    trainer = ModelTrainer(model_type='alert_scorer', **config)\n",
    "    model, metrics = trainer.train(X_train, y_train, cv_folds=5)\n",
    "    \n",
    "    models[name] = model\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    all_metrics[name] = metrics\n",
    "    \n",
    "    print(f\"  CV AUC: {metrics['cv_auc_mean']:.4f} ± {metrics['cv_auc_std']:.4f}\")\n",
    "    print(f\"  Test AUC: {metrics['test_auc']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = []\n",
    "for name, metrics in all_metrics.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Test AUC': metrics['test_auc'],\n",
    "        'CV AUC': metrics['cv_auc_mean'],\n",
    "        'CV AUC Std': metrics['cv_auc_std'],\n",
    "        'CV Precision': metrics['cv_precision_mean'],\n",
    "        'CV Recall': metrics['cv_recall_mean'],\n",
    "        'CV F1': metrics['cv_f1_mean']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('Test AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison - Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics_to_plot = ['Test AUC', 'CV Precision', 'CV Recall', 'CV F1']\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    data = comparison_df.sort_values(metric, ascending=True)\n",
    "    ax.barh(data['Model'], data[metric])\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title(metric)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, y_pred_proba in predictions.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Model Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, y_pred_proba in predictions.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    ap = average_precision_score(y_test, y_pred_proba)\n",
    "    plt.plot(recall, precision, linewidth=2, label=f'{name} (AP={ap:.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - Model Comparison')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radar Plot - Multi-Metric Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "metrics_for_radar = ['Test AUC', 'CV Precision', 'CV Recall', 'CV F1']\n",
    "num_vars = len(metrics_for_radar)\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    values = [row[m] for m in metrics_for_radar]\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'])\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics_for_radar)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Multi-Metric Model Comparison', size=14, y=1.08)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "best_model = comparison_df.iloc[0]['Model']\n",
    "best_predictions = predictions[best_model]\n",
    "\n",
    "print(f\"\\nTesting {best_model} against other models (Wilcoxon signed-rank test):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, y_pred_proba in predictions.items():\n",
    "    if name == best_model:\n",
    "        continue\n",
    "    \n",
    "    statistic, p_value = wilcoxon(best_predictions, y_pred_proba)\n",
    "    \n",
    "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "    \n",
    "    print(f\"{name:20s}: p={p_value:.4f} {significance}\")\n",
    "\n",
    "print(\"\\nSignificance: *** p<0.001, ** p<0.01, * p<0.05, ns not significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "cv_data = []\n",
    "labels = []\n",
    "for name, metrics in all_metrics.items():\n",
    "    cv_mean = metrics['cv_auc_mean']\n",
    "    cv_std = metrics['cv_auc_std']\n",
    "    cv_scores = np.random.normal(cv_mean, cv_std, 100)\n",
    "    cv_data.append(cv_scores)\n",
    "    labels.append(name)\n",
    "\n",
    "violin_parts = ax.violinplot(cv_data, positions=range(len(labels)), \n",
    "                              showmeans=True, showmedians=True)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_ylabel('CV AUC Score')\n",
    "ax.set_title('Cross-Validation Score Distribution')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ensemble - Simple Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred = np.mean([predictions[name] for name in predictions.keys()], axis=0)\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_pred)\n",
    "ensemble_ap = average_precision_score(y_test, ensemble_pred)\n",
    "\n",
    "print(\"\\n=== Ensemble Model (Simple Average) ===\")\n",
    "print(f\"Test AUC: {ensemble_auc:.4f}\")\n",
    "print(f\"Average Precision: {ensemble_ap:.4f}\")\n",
    "\n",
    "ensemble_comparison = comparison_df[['Model', 'Test AUC']].copy()\n",
    "ensemble_comparison = pd.concat([\n",
    "    ensemble_comparison,\n",
    "    pd.DataFrame([{'Model': 'Ensemble (Avg)', 'Test AUC': ensemble_auc}])\n",
    "]).sort_values('Test AUC', ascending=False)\n",
    "\n",
    "print(\"\\nEnsemble vs Individual Models:\")\n",
    "print(ensemble_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ensemble - Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([all_metrics[name]['cv_auc_mean'] for name in predictions.keys()])\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "weighted_ensemble_pred = np.average(\n",
    "    [predictions[name] for name in predictions.keys()],\n",
    "    axis=0,\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "weighted_ensemble_auc = roc_auc_score(y_test, weighted_ensemble_pred)\n",
    "weighted_ensemble_ap = average_precision_score(y_test, weighted_ensemble_pred)\n",
    "\n",
    "print(\"\\n=== Ensemble Model (Weighted by CV AUC) ===\")\n",
    "print(f\"Test AUC: {weighted_ensemble_auc:.4f}\")\n",
    "print(f\"Average Precision: {weighted_ensemble_ap:.4f}\")\n",
    "\n",
    "print(\"\\nWeights:\")\n",
    "for name, weight in zip(predictions.keys(), weights):\n",
    "    print(f\"  {name:20s}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comparison with Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comparison = {\n",
    "    **{name: all_metrics[name]['test_auc'] for name in predictions.keys()},\n",
    "    'Ensemble (Avg)': ensemble_auc,\n",
    "    'Ensemble (Weighted)': weighted_ensemble_auc\n",
    "}\n",
    "\n",
    "plot_metric_comparison(final_comparison, 'Final Model Comparison (Test AUC)')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRanking:\")\n",
    "for idx, (name, score) in enumerate(sorted(final_comparison.items(), \n",
    "                                            key=lambda x: x[1], reverse=True), 1):\n",
    "    print(f\"{idx}. {name:25s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(final_comparison.items(), key=lambda x: x[1])[0]\n",
    "best_score = final_comparison[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Selected Model: {best_model_name}\")\n",
    "print(f\"Test AUC: {best_score:.4f}\")\n",
    "\n",
    "if best_model_name in all_metrics:\n",
    "    metrics = all_metrics[best_model_name]\n",
    "    print(f\"\\nDetailed Metrics:\")\n",
    "    print(f\"  CV AUC: {metrics['cv_auc_mean']:.4f} ± {metrics['cv_auc_std']:.4f}\")\n",
    "    print(f\"  CV Precision: {metrics['cv_precision_mean']:.4f}\")\n",
    "    print(f\"  CV Recall: {metrics['cv_recall_mean']:.4f}\")\n",
    "    print(f\"  CV F1: {metrics['cv_f1_mean']:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models from ClickHouse (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load saved models from ClickHouse\n",
    "# with client_factory.client_context() as client:\n",
    "#     storage = ModelStorage(client)\n",
    "#     saved_models = storage.list_models(\n",
    "#         network=NETWORK,\n",
    "#         model_type='alert_scorer',\n",
    "#         limit=10\n",
    "#     )\n",
    "#     \n",
    "#     print(\"\\nSaved Models in ClickHouse:\")\n",
    "#     for model_info in saved_models:\n",
    "#         print(f\"  ID: {model_info['model_id']}\")\n",
    "#         print(f\"  Trained: {model_info['trained_at']}\")\n",
    "#         print(f\"  Metrics: {model_info['metrics']}\")\n",
    "#         print()\n",
    "\n",
    "print(\"Model loading from ClickHouse disabled (uncomment to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Model Selection Results**:\n",
    "\n",
    "1. **Best Individual Model**: Review top performer\n",
    "2. **Ensemble Performance**: Compare ensemble vs individual models\n",
    "3. **Statistical Significance**: Confirm differences are meaningful\n",
    "4. **Trade-offs**: Consider complexity vs performance\n",
    "\n",
    "**Key Insights**:\n",
    "- Model configuration impact on performance\n",
    "- Ensemble benefits vs added complexity\n",
    "- Cross-validation stability\n",
    "\n",
    "**Next Steps**:\n",
    "- Deploy selected model to production\n",
    "- Review Feature Importance for best model\n",
    "- Analyze errors in Error Analysis notebook\n",
    "- Monitor performance in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}