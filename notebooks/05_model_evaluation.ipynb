{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "**Purpose**: Comprehensive model evaluation and validation\n",
    "\n",
    "This notebook provides:\n",
    "- Multiple evaluation metrics\n",
    "- ROC and PR curves\n",
    "- Confusion matrices\n",
    "- Calibration analysis\n",
    "- Score distributions\n",
    "- Performance by segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from packages.training import FeatureExtractor, FeatureBuilder, ModelTrainer, ModelStorage\n",
    "from packages.storage import ClientFactory, get_connection_params\n",
    "from notebook_utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, average_precision_score,\n",
    "    precision_recall_fscore_support, log_loss, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK = 'ethereum'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-02-29'\n",
    "WINDOW_DAYS = 7\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Network: {NETWORK}\")\n",
    "print(f\"Evaluation Period: {START_DATE} to {END_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params = get_connection_params(NETWORK)\n",
    "client_factory = ClientFactory(connection_params)\n",
    "\n",
    "with client_factory.client_context() as client:\n",
    "    extractor = FeatureExtractor(client)\n",
    "    data = extractor.extract_training_data(\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        window_days=WINDOW_DAYS\n",
    "    )\n",
    "\n",
    "builder = FeatureBuilder()\n",
    "X, y = builder.build_training_features(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer(model_type='alert_scorer')\n",
    "model, metrics = trainer.train(X_train, y_train, cv_folds=5)\n",
    "\n",
    "print(\"Model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_proba = model.predict(X_train)\n",
    "y_test_pred_proba = model.predict(X_test)\n",
    "\n",
    "y_train_pred = (y_train_pred_proba > 0.5).astype(int)\n",
    "y_test_pred = (y_test_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"Predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "train_ap = average_precision_score(y_train, y_train_pred_proba)\n",
    "test_ap = average_precision_score(y_test, y_test_pred_proba)\n",
    "\n",
    "train_logloss = log_loss(y_train, y_train_pred_proba)\n",
    "test_logloss = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "train_brier = brier_score_loss(y_train, y_train_pred_proba)\n",
    "test_brier = brier_score_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CORE METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Metric':<20} {'Train':>12} {'Test':>12} {'Diff':>12}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'AUC':<20} {train_auc:>12.4f} {test_auc:>12.4f} {abs(train_auc-test_auc):>12.4f}\")\n",
    "print(f\"{'Average Precision':<20} {train_ap:>12.4f} {test_ap:>12.4f} {abs(train_ap-test_ap):>12.4f}\")\n",
    "print(f\"{'Log Loss':<20} {train_logloss:>12.4f} {test_logloss:>12.4f} {abs(train_logloss-test_logloss):>12.4f}\")\n",
    "print(f\"{'Brier Score':<20} {train_brier:>12.4f} {test_brier:>12.4f} {abs(train_brier-test_brier):>12.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Low Risk', 'High Risk']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_proba)\n",
    "\n",
    "ax1.plot(fpr_train, tpr_train, label=f'Train (AUC={train_auc:.3f})', linewidth=2)\n",
    "ax1.plot(fpr_test, tpr_test, label=f'Test (AUC={test_auc:.3f})', linewidth=2)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision_train, recall_train, _ = precision_recall_curve(y_train, y_train_pred_proba)\n",
    "precision_test, recall_test, _ = precision_recall_curve(y_test, y_test_pred_proba)\n",
    "\n",
    "ax2.plot(recall_train, precision_train, label=f'Train (AP={train_ap:.3f})', linewidth=2)\n",
    "ax2.plot(recall_test, precision_test, label=f'Test (AP={test_ap:.3f})', linewidth=2)\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_title('Training Set Confusion Matrix')\n",
    "ax1.set_xticklabels(['Low Risk', 'High Risk'])\n",
    "ax1.set_yticklabels(['Low Risk', 'High Risk'])\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_title('Test Set Confusion Matrix')\n",
    "ax2.set_xticklabels(['Low Risk', 'High Risk'])\n",
    "ax2.set_yticklabels(['Low Risk', 'High Risk'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].hist(y_train_pred_proba[y_train == 0], bins=50, alpha=0.6, label='Actual: Low', edgecolor='black')\n",
    "axes[0, 0].hist(y_train_pred_proba[y_train == 1], bins=50, alpha=0.6, label='Actual: High', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Predicted Probability')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Training Set: Score Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(y_test_pred_proba[y_test == 0], bins=50, alpha=0.6, label='Actual: Low', edgecolor='black')\n",
    "axes[0, 1].hist(y_test_pred_proba[y_test == 1], bins=50, alpha=0.6, label='Actual: High', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Predicted Probability')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Test Set: Score Distribution')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].boxplot([y_train_pred_proba[y_train == 0], y_train_pred_proba[y_train == 1]], \n",
    "                     labels=['Low Risk', 'High Risk'])\n",
    "axes[1, 0].set_ylabel('Predicted Probability')\n",
    "axes[1, 0].set_title('Training Set: Box Plot')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1, 1].boxplot([y_test_pred_proba[y_test == 0], y_test_pred_proba[y_test == 1]], \n",
    "                     labels=['Low Risk', 'High Risk'])\n",
    "axes[1, 1].set_ylabel('Predicted Probability')\n",
    "axes[1, 1].set_title('Test Set: Box Plot')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "prob_true_train, prob_pred_train = calibration_curve(y_train, y_train_pred_proba, n_bins=10)\n",
    "ax1.plot(prob_pred_train, prob_true_train, marker='o', linewidth=2, label='Model')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax1.set_xlabel('Mean Predicted Probability')\n",
    "ax1.set_ylabel('Fraction of Positives')\n",
    "ax1.set_title(f'Training Set Calibration (Brier={train_brier:.4f})')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "prob_true_test, prob_pred_test = calibration_curve(y_test, y_test_pred_proba, n_bins=10)\n",
    "ax2.plot(prob_pred_test, prob_true_test, marker='o', linewidth=2, label='Model')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax2.set_xlabel('Mean Predicted Probability')\n",
    "ax2.set_ylabel('Fraction of Positives')\n",
    "ax2.set_title(f'Test Set Calibration (Brier={test_brier:.4f})')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_test_pred_proba > threshold).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred_thresh, average='binary', zero_division=0\n",
    "    )\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precision_scores, marker='o', label='Precision', linewidth=2)\n",
    "plt.plot(thresholds, recall_scores, marker='s', label='Recall', linewidth=2)\n",
    "plt.plot(thresholds, f1_scores, marker='^', label='F1 Score', linewidth=2)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metrics vs Classification Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "print(f\"\\nBest threshold for F1: {thresholds[best_f1_idx]:.2f}\")\n",
    "print(f\"  Precision: {precision_scores[best_f1_idx]:.4f}\")\n",
    "print(f\"  Recall: {recall_scores[best_f1_idx]:.4f}\")\n",
    "print(f\"  F1: {f1_scores[best_f1_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': y_test_pred_proba\n",
    "})\n",
    "test_df = test_df.sort_values('predicted', ascending=False).reset_index(drop=True)\n",
    "\n",
    "deciles = 10\n",
    "test_df['decile'] = pd.qcut(test_df.index, deciles, labels=False, duplicates='drop') + 1\n",
    "\n",
    "lift_data = test_df.groupby('decile').agg({\n",
    "    'actual': ['sum', 'count']\n",
    "}).reset_index()\n",
    "lift_data.columns = ['decile', 'positives', 'total']\n",
    "lift_data['positive_rate'] = lift_data['positives'] / lift_data['total']\n",
    "\n",
    "baseline_rate = y_test.mean()\n",
    "lift_data['lift'] = lift_data['positive_rate'] / baseline_rate\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.bar(lift_data['decile'], lift_data['positive_rate'], alpha=0.7)\n",
    "ax1.axhline(y=baseline_rate, color='r', linestyle='--', label=f'Baseline: {baseline_rate:.3f}')\n",
    "ax1.set_xlabel('Decile')\n",
    "ax1.set_ylabel('Positive Rate')\n",
    "ax1.set_title('Positive Rate by Score Decile')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "ax2.bar(lift_data['decile'], lift_data['lift'], alpha=0.7)\n",
    "ax2.axhline(y=1.0, color='r', linestyle='--', label='Baseline')\n",
    "ax2.set_xlabel('Decile')\n",
    "ax2.set_ylabel('Lift')\n",
    "ax2.set_title('Lift by Score Decile')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLift Analysis:\")\n",
    "print(lift_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics = {\n",
    "    'AUC': [metrics['cv_auc_mean'], metrics['cv_auc_std']],\n",
    "    'Precision': [metrics['cv_precision_mean'], metrics['cv_precision_std']],\n",
    "    'Recall': [metrics['cv_recall_mean'], metrics['cv_recall_std']],\n",
    "    'F1': [metrics['cv_f1_mean'], metrics['cv_f1_std']]\n",
    "}\n",
    "\n",
    "cv_df = pd.DataFrame(cv_metrics, index=['Mean', 'Std']).T\n",
    "print(\"\\nCross-Validation Metrics:\")\n",
    "print(cv_df.round(4))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(cv_df))\n",
    "ax.bar(x, cv_df['Mean'], yerr=cv_df['Std'], capsize=5, alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cv_df.index)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Cross-Validation Metrics (Mean ± Std)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'Metric': ['AUC', 'Average Precision', 'Log Loss', 'Brier Score', 'Test Accuracy'],\n",
    "    'Train': [\n",
    "        train_auc,\n",
    "        train_ap,\n",
    "        train_logloss,\n",
    "        train_brier,\n",
    "        (y_train_pred == y_train).mean()\n",
    "    ],\n",
    "    'Test': [\n",
    "        test_auc,\n",
    "        test_ap,\n",
    "        test_logloss,\n",
    "        test_brier,\n",
    "        (y_test_pred == y_test).mean()\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df['Overfitting'] = summary_df['Train'] - summary_df['Test']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**Model Performance**:\n",
    "- Review AUC, precision, recall, and F1 scores\n",
    "- Check train/test gap for overfitting\n",
    "- Analyze calibration quality\n",
    "\n",
    "**Key Observations**:\n",
    "- Calibration curve shows prediction reliability\n",
    "- Lift chart demonstrates model value\n",
    "- Threshold analysis helps optimize for business needs\n",
    "\n",
    "**Next Steps**:\n",
    "- Compare with other models in Model Comparison notebook\n",
    "- Analyze errors in Error Analysis notebook\n",
    "- Review feature importance for insights\n",
    "- Consider deployment if performance is satisfactory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}